import numpy as np
import math
import scipy.stats as stats
from abc import ABCMeta, abstractmethod
import distributions 
import utils_math
from mpl_toolkits.mplot3d import Axes3D
import matplotlib.pyplot as plt
import time


    
class GaussianToy(object):

    '''
    A toy problem where data is generated by a N-D Gaussian 
        x ~ p(x|theta)
        theta = mu (for K dimensions), rho
    '''

    def __init__(self, N=100, n=50, dim=2, rho=0.0):
        self.N = N                                                                       # number of posterior samples
        self.n = n                                                                       # number of i.i.d data x_i ~ p(x|theta)
        self.stat = 'raw'                                                                # use raw stat or expert stat
        
        self.simulator_args = ['mu_per_dim']                                             # just for information
        self.K = dim                                                                     # number of parameters
        
        self.prior = np.array([*(distributions.uniform for i in range(self.K))])                      
        self.prior_args = np.array([*([-0.5,1.0] for i in range(self.K))])
        
        self.rho = rho
        self.cov = (np.zeros((dim, dim))+self.rho + np.eye(dim)*(1-self.rho))
        self.mu_arrays = np.linspace(0,1,dim+2)[1:dim+1]*0 + 0.50
        
        print('self.cov=', self.cov, 'valid?', np.all(np.linalg.eigvals(self.cov) > 0))
        print('self.prior_args', self.prior_args)
        print('self.mu_arrays', self.mu_arrays)
                
    def get_true_theta(self):
        return np.array(list(self.mu_arrays))

    # A. (normalized) marginal quantiles
    def _ss_quantiles(self, X, n_quantiles):
        dim = X.shape[1]
        prob = np.linspace(0.025, 0.975, n_quantiles)
        stat = np.zeros([1, n_quantiles*dim])
        for k in range(dim):
            quantiles = stats.mstats.mquantiles(X[:, k], prob)
            stat_k = quantiles
            stat[0, k*n_quantiles : (k+1)*n_quantiles] = np.array(stat_k)
        return stat

    # B. correlation between latent
    def _ss_corr(self, Z):
        V = np.mat(Z).T * np.mat(Z) / Z.shape[0]
        (d,d) = V.shape
        upper_tri_elements = V[np.triu_indices(d, k=1)]
        stat = np.array(upper_tri_elements)
        return stat
    
    def statistics(self, data, types='expert'):  
        data = data.reshape((1, -1))
        n_quantiles = 12
        stat_A = data.reshape(-1, n_quantiles)   
        if types == 'sufficient':
            # true sufficient statistics (data mean)
            stat = np.mean(stat_A, axis=1, keepdims=False).reshape((1, -1))
            return stat.reshape((1, -1))
        if types == 'expert':       
            # expert statistics, but insufficient
            stat = np.mean(stat_A, axis=1, keepdims=False).reshape((1, -1))
            #mask = np.ones((1, self.K))
            #mask[:, 0:int(self.K/2)] = 0
            #stat = stat*mask  
            stat = stat[:, 0:int(self.K/2)]
        if types == 'expert2': 
            # expert statistics, but insufficient
            stat = np.mean(stat_A, axis=1, keepdims=False).reshape((1, -1))
            mask = np.ones((1, self.K))
            mask[:, int(self.K/2):] = 0
            stat = stat*mask  
        return stat
        
    def sample_data(self, theta):
        # some preparation
        dim = self.K
        mu = theta[0:dim]
        #rho = theta[-1]
        rho = self.rho
        V = (np.zeros((dim, dim))+rho + np.eye(dim)*(1-rho))
        # sample Z ~ N(mu, V)
        X = distributions.normal_nd.draw_samples(mu, V, self.n)
        # extract low-level statistics
        data = X
        stat_A = self._ss_quantiles(data, n_quantiles=12)
        stat_B = self._ss_corr(data)
        return stat_A

    def log_likelihood(self, theta):

        # calculate L(theta; x_o) = p(theta|x)
        
        log_pdf = self.log_pdf(self.data_obs, theta)
        return log_pdf
    
    def log_pdf(self, data, theta):
        # calculate p(x|theta) 
        dim = self.K
        mu = theta[0:dim]
        rho = theta[-1]
        V = (np.zeros((dim, dim))+rho + np.eye(dim)*(1-rho))
        log_ll = distributions.normal_nd.logpdf(self.data_obs, mu, V)
        return log_ll.sum()

    def sample_from_prior(self):
        mu_array = []
        for i in range(self.K):
            mu = self.prior[i].draw_samples(self.prior_args[i, 0], self.prior_args[i, 1],  1)[0]
            mu_array.append(mu)
        return np.array(mu_array)

    
    
    
class Toy_Problem2(object):

    '''
    A toy problem where data is generated by a N-D Gaussian 
        x ~ p(x|theta)
        s_k = f_k(x)
        y = g(x)
    '''

    def __init__(self, N=100, n=50, x_dim=1):
        self.N = N                                                                       # number of posterior samples
        self.n = n                                                                       # number of i.i.d data x_i ~ p(x|theta)
        self.stat = 'raw'                                                                # use raw stat or expert stat
        
        self.x_dim = x_dim
        self.z_dim = self.x_dim
        self.y_dim = self.x_dim
        self.data_dim = 3*self.x_dim
        self.y_func = 'x1 + x2 + x3**2'

#     def statistics(self, data, theta=None):
#         if self.stat == 'raw':
#             stat = data
#             return stat    
#         if self.stat == 'expert':
#             x1, x2, x3 = self.get_x_from_data(data)
#             return x1 + x2
#         if self.stat == 'expert2':
#             x1, x2, x3 = self.get_x_from_data(data)
#             return x3**2
        
    def statistics(self, data, theta=None):
        x1, x2, x3 = self.get_x_from_data(data)
        s1 = x1 + x2
        s2 = x3**2
        return s1, s2
        
    def get_x_from_data(self, data):
        i = 0
        x1 = data[0,int(self.x_dim*i):int(self.x_dim*(i+1))]
        x2 = data[0,int(self.x_dim*(i+1)):int(self.x_dim*(i+2))]
        x3 = data[0,int(self.x_dim*(i+2)):int(self.x_dim*(i+3))]
        return x1, x2, x3
    
    def y(self, data):
        x1, x2, x3 = self.get_x_from_data(data)
        if self.y_func == 'x1 + x2 + x3**2':
            return (x1 + x2) + x3**2
        if self.y_func == '(x1 + x2) * x3**2':
            return (x1 + x2) * (x3**2)
        if self.y_func == 'x1*x2*(x3**2) + x3**2':
            return x1*x2*(x3**2) + x3**2
        if self.y_func == 'x1 + x2**2':
            return x1 + x2**2
        
    def sample_data(self, theta=None):
        # some preparation
        data = np.zeros((1, self.data_dim))
        for i in range(3):
            a,b = int(self.x_dim*i), int(self.x_dim*(i+1))
            #data[0,a:b] = distributions.normal_nd.draw_samples(np.zeros(self.x_dim), np.eye(self.x_dim), 1)
            data[0,a:b] = distributions.beta.draw_samples(2, 2, self.x_dim)
        return data