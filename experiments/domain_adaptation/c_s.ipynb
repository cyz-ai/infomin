{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7f3ca51-a30a-4608-ab27-eb7b051c2a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "parent_dir = os.path.abspath('../..')\n",
    "\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.append(parent_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e94f3b-cb0c-40d6-8962-50b6230a3975",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a27c01e9-b729-4387-8725-d799f8bc43b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from itertools import cycle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "from torchvision.utils import make_grid\n",
    "import torchvision.datasets as dset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import utils_os\n",
    "import mi\n",
    "\n",
    "\n",
    "torch.manual_seed(time.time())\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9704f7cd-f8f2-4d51-a988-a228fd877111",
   "metadata": {},
   "source": [
    "# Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c75eb6da-61d2-4359-ace5-a732635c3dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cuda:0' \n",
    "\n",
    "class Hyperparams(utils_os.ConfigDict):\n",
    "\n",
    "    def __init__(self): \n",
    "        self.device = DEVICE\n",
    "        self.learning_rate = 1e-3\n",
    "        self.n_epochs = 100\n",
    "        self.grad_clip = None\n",
    "        self.batch_size = 128\n",
    "        self.image_shape = (32, 32)\n",
    "        self.update_inner_every_num_epoch = 2\n",
    "\n",
    "BASE_HYPERPARAMS = Hyperparams()\n",
    "\n",
    "ROOT = '../..'\n",
    "MODEL_SAVE_DIR = f'{ROOT}/results/c_s'\n",
    "DATASET_DIR = f'{ROOT}/data'\n",
    "\n",
    "NUM_COLOR_CHANNELS = 3\n",
    "NUM_CLASSES = 9\n",
    "NUM_DOMAIN = 2\n",
    "\n",
    "DIM_Z_CONTENT, DIM_Z_DOMAIN = 128, 128\n",
    "\n",
    "INFOMIN_TRAIN_BATCH_SIZE = 2500"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1227c6e7-fd3e-4638-90c0-9fe820d4c2d3",
   "metadata": {},
   "source": [
    "# Prepare Dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276ca091-ff6e-49d5-b389-db55db97ac09",
   "metadata": {},
   "source": [
    "### Align labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51fef6c1-355f-4e87-b977-b7db0a3d287f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CIFAR\n",
    "0: plane\n",
    "1: car\n",
    "2: bird\n",
    "3: cat\n",
    "4: deer\n",
    "5: dog\n",
    "6: frog\n",
    "7: horse\n",
    "8: ship\n",
    "9: truck\n",
    "\n",
    "remove frog(6)\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "STL\n",
    "0: plane\n",
    "1: bird\n",
    "2: car\n",
    "3: cat\n",
    "4: deer\n",
    "5: dog\n",
    "6: horse\n",
    "7: monkey\n",
    "8: ship\n",
    "9: truck\n",
    "\n",
    "remove monkey(7)\n",
    "\"\"\"\n",
    "CIFAR_MAP = {\n",
    "    0: 0,\n",
    "    1: 1,\n",
    "    2: 2,\n",
    "    3: 3,\n",
    "    4: 4,\n",
    "    5: 5,\n",
    "    6: -2,\n",
    "    7: 6,\n",
    "    8: 7,\n",
    "    9: 8,\n",
    "    -1: -1,\n",
    "}\n",
    "\n",
    "STL_MAP = {\n",
    "    0: 0,\n",
    "    1: 2,\n",
    "    2: 1,\n",
    "    3: 3,\n",
    "    4: 4,\n",
    "    5: 5,\n",
    "    6: 6,\n",
    "    7: -2,\n",
    "    8: 7,\n",
    "    9: 8,\n",
    "    -1: -1,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d691f34c-77ab-42c1-9de9-c5c1a8169b2d",
   "metadata": {},
   "source": [
    "### Load CIFAR10 and STL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "104b8bab-0236-4147-af24-089467f1cfb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(DATASET_DIR):\n",
    "    os.mkdir(DATASET_DIR)\n",
    "    \n",
    "# define data transformations\n",
    "trans_train = transforms.Compose([\n",
    "    transforms.Resize(32),\n",
    "    transforms.RandomCrop(32, padding=4, padding_mode='reflect'), \n",
    "    transforms.RandomHorizontalFlip(), \n",
    "    transforms.ToTensor(), \n",
    "])\n",
    "trans_test = transforms.Compose([\n",
    "    transforms.Resize(32),\n",
    "    transforms.ToTensor(), \n",
    "])\n",
    "target_trans_cifar = transforms.Compose([lambda x: CIFAR_MAP[x]])\n",
    "target_trans_stl = transforms.Compose([lambda x: STL_MAP[x]])\n",
    "\n",
    "\n",
    "# load cifar (remove the class 'frog')\n",
    "m_train_set = dset.CIFAR10(root=DATASET_DIR, train=True, transform=trans_train, target_transform=target_trans_cifar, download=True)\n",
    "m_test_set = dset.CIFAR10(root=DATASET_DIR, train=False, transform=trans_test, target_transform=target_trans_cifar, download=True)\n",
    "\n",
    "cifar_train_indices = [idx for idx, (_, target) in enumerate(m_train_set) if target != -2]\n",
    "m_train_loader = torch.utils.data.DataLoader(\n",
    "                 dataset=torch.utils.data.Subset(m_train_set, cifar_train_indices),\n",
    "                 batch_size=BASE_HYPERPARAMS.batch_size)\n",
    "cifar_test_indices = [idx for idx, (_, target) in enumerate(m_test_set) if target != -2]\n",
    "m_test_loader = torch.utils.data.DataLoader(\n",
    "                dataset=torch.utils.data.Subset(m_test_set, cifar_test_indices),\n",
    "                batch_size=BASE_HYPERPARAMS.batch_size,\n",
    "                shuffle=False)\n",
    "domain1 = 'CIFAR'\n",
    "\n",
    "# load stl-10 (remove the class 'monkey')\n",
    "mm_train_set = dset.STL10(root=DATASET_DIR, split='train', transform=trans_train, target_transform=target_trans_stl, download=True)\n",
    "mm_test_set = dset.STL10(root=DATASET_DIR, split='test', transform=trans_test, target_transform=target_trans_stl, download=True)\n",
    "\n",
    "stl_train_indices = [idx for idx, (_, target) in enumerate(mm_train_set) if target != -2]\n",
    "mm_train_loader = torch.utils.data.DataLoader(\n",
    "                 dataset=torch.utils.data.Subset(mm_train_set, stl_train_indices),\n",
    "                 batch_size=BASE_HYPERPARAMS.batch_size)\n",
    "stl_test_indices = [idx for idx, (_, target) in enumerate(mm_test_set) if target != -2]\n",
    "mm_test_loader = torch.utils.data.DataLoader(\n",
    "                dataset=torch.utils.data.Subset(mm_test_set, stl_test_indices),\n",
    "                batch_size=BASE_HYPERPARAMS.batch_size,\n",
    "                shuffle=False)  \n",
    "domain2 = 'STL'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409ddba7-48b2-4a38-a8c1-833179597cff",
   "metadata": {},
   "source": [
    "### Load all data into memeory for faster access when training infomin layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ac7b090-8544-4502-8802-2e664d2be53a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loader2tensor finished. data max= tensor(1.) data min= tensor(0.)\n",
      "loader2tensor finished. data max= tensor(1.) data min= tensor(0.)\n",
      "loader2tensor finished. data max= tensor(1.) data min= tensor(0.)\n",
      "loader2tensor finished. data max= tensor(1.) data min= tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "def loader2tensor(dataloader):\n",
    "    all_data, all_target = [], []\n",
    "    for data, target in iter(dataloader):\n",
    "        all_data.append(data)\n",
    "        all_target.append(target)\n",
    "    print('loader2tensor finished.', 'data max=', data.max(), 'data min=', data.min())\n",
    "    return torch.cat(all_data, dim=0), torch.cat(all_target, dim=0)\n",
    "\n",
    "all_data_m, all_label_m = loader2tensor(m_train_loader)\n",
    "all_data_test_m, all_label_test_m = loader2tensor(m_test_loader)\n",
    "\n",
    "all_data_mm, all_label_mm = loader2tensor(mm_train_loader)\n",
    "all_data_test_mm, all_label_test_mm = loader2tensor(mm_test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3f1990-ded3-4185-9b3c-5106ec124271",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21bbe588-1fe7-4049-b209-6cb05b9cdf6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tasks.domain_adaptation import DomainAdaptation, train, test\n",
    "from routine import exp_run\n",
    "\n",
    "\n",
    "def inner_batch_provider():\n",
    "    idx1, idx2 = torch.randperm(len(all_data_m)), torch.randperm(len(all_data_mm))\n",
    "    infomin_x1, infomin_x2 = all_data_m[idx1[:INFOMIN_TRAIN_BATCH_SIZE]].to(DEVICE), all_data_mm[idx2[:INFOMIN_TRAIN_BATCH_SIZE]].to(DEVICE)\n",
    "\n",
    "    return infomin_x1, infomin_x2\n",
    "\n",
    "\n",
    "def get_model_path(hyperparams, epoch=None):\n",
    "    if epoch is not None:\n",
    "        return f'{MODEL_SAVE_DIR}/{hyperparams.estimator}/model_{hyperparams.get_name(\"alpha\", \"beta\", \"gamma\")}_{epoch}'\n",
    "    return f'{MODEL_SAVE_DIR}/{hyperparams.estimator}/model_{hyperparams.get_name(\"alpha\", \"beta\", \"gamma\")}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8b8423f-d5c5-4bde-a86a-54814aac9458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 2.1747948821161835, loss_content: 1.4733088839222008, loss_domain: 0.6865523583452466 redundancy: 0.049778826521630855  acc_d1: 51.56666564941406 acc_d2: 45.00667190551758 acc_domain: 55.43019104003906\n",
      "\n",
      "epoch: 1, loss: 1.450339114162284, loss_content: 0.766580473369276, loss_domain: 0.669143989052571 redundancy: 0.048715557142252654  acc_d1: 72.96666717529297 acc_d2: 61.97731399536133 acc_domain: 58.78723907470703\n",
      "\n",
      "epoch: 2, loss: 0.9842295470372052, loss_content: 0.5789589516713586, loss_domain: 0.38811774321005377 redundancy: 0.05717618944792365  acc_d1: 79.47777557373047 acc_d2: 68.1272201538086 acc_domain: 82.23099517822266\n",
      "\n",
      "epoch: 3, loss: 0.8458467277003007, loss_content: 0.6221673480221923, loss_domain: 0.20840076871321234 redundancy: 0.050928688732723534  acc_d1: 78.97777557373047 acc_d2: 63.80115509033203 acc_domain: 91.92974853515625\n",
      "\n",
      "epoch: 4, loss: 0.7643178852511124, loss_content: 0.576227991094052, loss_domain: 0.17206421945716294 redundancy: 0.05341890115868038  acc_d1: 80.04444885253906 acc_d2: 68.52758026123047 acc_domain: 93.84170532226562\n",
      "\n",
      "epoch: 5, loss: 1.3815042443678414, loss_content: 0.577463020321349, loss_domain: 0.7865199958774406 redundancy: 0.058404077513432  acc_d1: 80.77777862548828 acc_d2: 67.09297180175781 acc_domain: 73.59381866455078\n",
      "\n",
      "epoch: 6, loss: 0.8348858213760484, loss_content: 0.47915491629654255, loss_domain: 0.3378026909391645 redundancy: 0.05976071597640993  acc_d1: 83.78888702392578 acc_d2: 69.57295227050781 acc_domain: 89.23966217041016\n",
      "\n",
      "epoch: 7, loss: 0.7539145232925952, loss_content: 0.42816972039954765, loss_domain: 0.28799003761419106 redundancy: 0.12584921994417067  acc_d1: 85.37777709960938 acc_d2: 72.94261169433594 acc_domain: 91.03490447998047\n",
      "\n",
      "epoch: 8, loss: 0.7468037823556175, loss_content: 0.46238566032597717, loss_domain: 0.2683949025583939 redundancy: 0.053410731656083336  acc_d1: 84.80000305175781 acc_d2: 71.40792083740234 acc_domain: 89.801025390625\n",
      "\n",
      "epoch: 9, loss: 1.0531942281924502, loss_content: 0.6801653966097765, loss_domain: 0.35758547442899624 redundancy: 0.0514778318080369  acc_d1: 80.0 acc_d2: 67.3709945678711 acc_domain: 90.62361145019531\n",
      "\n",
      "epoch: 10, loss: 1.5906489902818706, loss_content: 0.4255704078036295, loss_domain: 1.1473094157769645 redundancy: 0.059230545745559145  acc_d1: 86.5111083984375 acc_d2: 73.6877212524414 acc_domain: 69.51978302001953\n",
      "\n",
      "epoch: 11, loss: 0.8177545633114559, loss_content: 0.36448366247432334, loss_domain: 0.4360613522815033 redundancy: 0.05736515887448905  acc_d1: 87.83333587646484 acc_d2: 76.25667572021484 acc_domain: 89.78990936279297\n",
      "\n",
      "epoch: 12, loss: 0.76350831313872, loss_content: 0.34038170668440804, loss_domain: 0.40494139807325014 redundancy: 0.060617346513334294  acc_d1: 88.75555419921875 acc_d2: 77.74688720703125 acc_domain: 86.6718521118164\n",
      "\n",
      "epoch: 13, loss: 0.5455942636644336, loss_content: 0.35121467331765405, loss_domain: 0.17543153338868853 redundancy: 0.0631601853610974  acc_d1: 88.80000305175781 acc_d2: 77.03514099121094 acc_domain: 94.75878143310547\n",
      "\n",
      "epoch: 14, loss: 2.4647125417078044, loss_content: 0.34739847544213415, loss_domain: 2.100040866455562 redundancy: 0.05757736084117016  acc_d1: 88.75555419921875 acc_d2: 77.84697723388672 acc_domain: 70.34793090820312\n",
      "\n",
      "epoch: 15, loss: 0.9551072523627483, loss_content: 0.36782333149876395, loss_domain: 0.567201726033654 redundancy: 0.066940648881325  acc_d1: 88.68888854980469 acc_d2: 76.57917785644531 acc_domain: 88.76722717285156\n",
      "\n",
      "epoch: 16, loss: 1.157909971727452, loss_content: 0.3397992811572384, loss_domain: 0.8032365399347224 redundancy: 0.04958048685860466  acc_d1: 89.21111297607422 acc_d2: 77.44661712646484 acc_domain: 87.74455261230469\n",
      "\n",
      "epoch: 17, loss: 0.5465177259814571, loss_content: 0.32012680739584104, loss_domain: 0.2074136349936606 redundancy: 0.06325761944344135  acc_d1: 89.81111145019531 acc_d2: 77.68016052246094 acc_domain: 94.8199234008789\n",
      "\n",
      "epoch: 18, loss: 0.5248207361765311, loss_content: 0.32482383158844963, loss_domain: 0.18575392495578444 redundancy: 0.04747659796980065  acc_d1: 90.05555725097656 acc_d2: 78.98131561279297 acc_domain: 95.68141174316406\n",
      "\n",
      "epoch: 19, loss: 5.6279330287181155, loss_content: 0.3432804979908634, loss_domain: 5.264184615981411 redundancy: 0.06822644874558483  acc_d1: 90.03333282470703 acc_d2: 78.06939697265625 acc_domain: 63.50044631958008\n",
      "\n",
      "epoch: 20, loss: 1.0493870914821894, loss_content: 0.3418940011884125, loss_domain: 0.6910166110790951 redundancy: 0.054921573624801885  acc_d1: 89.96666717529297 acc_d2: 78.74777221679688 acc_domain: 82.16429901123047\n",
      "\n",
      "epoch: 21, loss: 1.4037178536535988, loss_content: 0.3663573229396847, loss_domain: 1.01989587176014 redundancy: 0.05821553236004752  acc_d1: 89.30000305175781 acc_d2: 78.5587158203125 acc_domain: 83.75944519042969\n",
      "\n",
      "epoch: 22, loss: 8.405688151507311, loss_content: 0.3724378420853279, loss_domain: 8.015732140608236 redundancy: 0.058393962930520654  acc_d1: 89.3888931274414 acc_d2: 78.4697494506836 acc_domain: 52.1120491027832\n",
      "\n",
      "epoch: 23, loss: 0.5197754372173632, loss_content: 0.35613890894701783, loss_domain: 0.14518268673982418 redundancy: 0.061512807764532704  acc_d1: 89.86666870117188 acc_d2: 77.91370391845703 acc_domain: 95.63139343261719\n",
      "\n",
      "epoch: 24, loss: 1.135134861502849, loss_content: 0.3505238439415542, loss_domain: 0.7683798162870004 redundancy: 0.054103986884821466  acc_d1: 89.96666717529297 acc_d2: 79.1147689819336 acc_domain: 89.00066375732422\n",
      "\n",
      "epoch: 25, loss: 0.6093438918321905, loss_content: 0.34525693638224, loss_domain: 0.24853931577272817 redundancy: 0.05182548668760229  acc_d1: 90.27777862548828 acc_d2: 78.22509002685547 acc_domain: 95.1256103515625\n",
      "\n",
      "epoch: 26, loss: 1.0867000155045952, loss_content: 0.3426600988482086, loss_domain: 0.7288902926612908 redundancy: 0.0504987504411961  acc_d1: 90.56666564941406 acc_d2: 79.24822235107422 acc_domain: 88.66718292236328\n",
      "\n",
      "epoch: 27, loss: 0.6470058988517439, loss_content: 0.3285703409305761, loss_domain: 0.2983664475696188 redundancy: 0.06689704087575976  acc_d1: 90.65555572509766 acc_d2: 79.40391540527344 acc_domain: 92.646728515625\n",
      "\n",
      "epoch: 28, loss: 1.2867378441380783, loss_content: 0.3537962147047822, loss_domain: 0.9161969385516475 redundancy: 0.0558156631165512  acc_d1: 90.46666717529297 acc_d2: 79.19261169433594 acc_domain: 82.07537078857422\n",
      "\n",
      "epoch: 29, loss: 0.6884589069326159, loss_content: 0.34026834494631053, loss_domain: 0.33244944437288904 redundancy: 0.0524703958530871  acc_d1: 90.85556030273438 acc_d2: 79.80426788330078 acc_domain: 91.8741683959961\n",
      "\n",
      "epoch: 30, loss: 0.5457678710071134, loss_content: 0.3663149295558392, loss_domain: 0.1600568335450871 redundancy: 0.06465368900238208  acc_d1: 90.63333129882812 acc_d2: 79.8265151977539 acc_domain: 96.03157043457031\n",
      "\n",
      "epoch: 31, loss: 1.038786663136012, loss_content: 0.344188831851516, loss_domain: 0.6777972599989931 redundancy: 0.056001891375479985  acc_d1: 90.66666412353516 acc_d2: 79.55960845947266 acc_domain: 89.52312469482422\n",
      "\n",
      "epoch: 32, loss: 0.6571503802084587, loss_content: 0.3683141971557913, loss_domain: 0.2716641513184762 redundancy: 0.057240099121462294  acc_d1: 90.5999984741211 acc_d2: 79.81539154052734 acc_domain: 94.78101348876953\n",
      "\n",
      "epoch: 33, loss: 0.7457389378211867, loss_content: 0.3484568973662148, loss_domain: 0.37288172857862123 redundancy: 0.08133437173624694  acc_d1: 91.15555572509766 acc_d2: 80.10453796386719 acc_domain: 93.38594818115234\n",
      "\n",
      "epoch: 34, loss: 0.9761510898529644, loss_content: 0.3394660985385868, loss_domain: 0.6125841976051599 redundancy: 0.08033597519175267  acc_d1: 91.0777816772461 acc_d2: 79.77090454101562 acc_domain: 92.5855941772461\n",
      "\n",
      "epoch: 35, loss: 0.8700792789459229, loss_content: 0.3576116754975117, loss_domain: 0.4979855327958792 redundancy: 0.048273558111887584  acc_d1: 91.26667022705078 acc_d2: 79.48175811767578 acc_domain: 92.10204315185547\n",
      "\n",
      "epoch: 36, loss: 0.535629818137263, loss_content: 0.31856450900225575, loss_domain: 0.20365278590732896 redundancy: 0.044708406736313454  acc_d1: 91.20000457763672 acc_d2: 80.44928741455078 acc_domain: 95.52578735351562\n",
      "\n",
      "epoch: 37, loss: 0.5564758106016777, loss_content: 0.36687996815627727, loss_domain: 0.1744009877594424 redundancy: 0.050649530372359385  acc_d1: 90.92222595214844 acc_d2: 79.80426788330078 acc_domain: 95.89261627197266\n",
      "\n",
      "epoch: 38, loss: 2.6704615757498944, loss_content: 0.3678935451406828, loss_domain: 2.272781364514794 redundancy: 0.09928889817912394  acc_d1: 90.76667022705078 acc_d2: 79.81539154052734 acc_domain: 59.67652130126953\n",
      "\n",
      "epoch: 39, loss: 0.8619790295480003, loss_content: 0.359789167071732, loss_domain: 0.48540188833861286 redundancy: 0.05595990733414049  acc_d1: 90.54444885253906 acc_d2: 79.79315185546875 acc_domain: 90.31791687011719\n",
      "\n",
      "epoch: 40, loss: 0.8314197453814494, loss_content: 0.3508910586086797, loss_domain: 0.4601059098478774 redundancy: 0.0680759273285807  acc_d1: 91.4111099243164 acc_d2: 79.47064208984375 acc_domain: 93.92507934570312\n",
      "\n",
      "epoch: 41, loss: 0.573904554608842, loss_content: 0.35110913815212924, loss_domain: 0.20421695898116474 redundancy: 0.06192819204863528  acc_d1: 91.65555572509766 acc_d2: 79.81539154052734 acc_domain: 95.4479751586914\n",
      "\n",
      "epoch: 42, loss: 0.5089529840879037, loss_content: 0.3658180222125121, loss_domain: 0.12702053575448588 redundancy: 0.0537147445236923  acc_d1: 90.83333587646484 acc_d2: 79.6152114868164 acc_domain: 96.87083435058594\n",
      "\n",
      "epoch: 43, loss: 0.5420638247275017, loss_content: 0.3410347447219029, loss_domain: 0.18328018869522592 redundancy: 0.05916298110641434  acc_d1: 91.5111083984375 acc_d2: 80.1712646484375 acc_domain: 96.29279327392578\n",
      "\n",
      "epoch: 44, loss: 0.6360368220738961, loss_content: 0.39727329221409813, loss_domain: 0.2229022008012718 redundancy: 0.0528710950165987  acc_d1: 90.63333129882812 acc_d2: 80.30471801757812 acc_domain: 96.46509552001953\n",
      "\n",
      "epoch: 45, loss: 0.6380780477758864, loss_content: 0.41583153846817955, loss_domain: 0.19902973947390704 redundancy: 0.07738922607206122  acc_d1: 90.17778015136719 acc_d2: 78.5587158203125 acc_domain: 96.44286346435547\n",
      "\n",
      "epoch: 46, loss: 0.7207951684233168, loss_content: 0.3999375648481745, loss_domain: 0.3018732481229473 redundancy: 0.06328118426239931  acc_d1: 91.22222137451172 acc_d2: 80.63834381103516 acc_domain: 93.39706420898438\n",
      "\n",
      "epoch: 47, loss: 0.573127527471999, loss_content: 0.3579621069448095, loss_domain: 0.20020538169733235 redundancy: 0.049866797625493835  acc_d1: 91.36666870117188 acc_d2: 80.2602310180664 acc_domain: 96.00377655029297\n",
      "\n",
      "epoch: 48, loss: 1.392275505502459, loss_content: 0.3745653056342837, loss_domain: 1.0021798661057377 redundancy: 0.051767782006465216  acc_d1: 91.61111450195312 acc_d2: 80.39368438720703 acc_domain: 88.65606689453125\n",
      "\n",
      "epoch: 49, loss: 0.7552409360946064, loss_content: 0.3999704185505988, loss_domain: 0.33688387001903963 redundancy: 0.061288847968402047  acc_d1: 91.15555572509766 acc_d2: 79.34831237792969 acc_domain: 96.26500701904297\n",
      "\n",
      "epoch: 50, loss: 0.7010246834284822, loss_content: 0.33643747635290655, loss_domain: 0.3510782009279224 redundancy: 0.04503000230515297  acc_d1: 92.35556030273438 acc_d2: 81.83940887451172 acc_domain: 95.43130493164062\n",
      "\n",
      "epoch: 51, loss: 0.634454330508138, loss_content: 0.3267776244123217, loss_domain: 0.28926368152171794 redundancy: 0.06137674562895382  acc_d1: 92.5111083984375 acc_d2: 81.9172592163086 acc_domain: 96.03712463378906\n",
      "\n",
      "epoch: 52, loss: 0.6599329936672265, loss_content: 0.32524683043150837, loss_domain: 0.31890824027884174 redundancy: 0.052593058040759096  acc_d1: 92.68888854980469 acc_d2: 81.96174621582031 acc_domain: 95.91484832763672\n",
      "\n",
      "epoch: 53, loss: 0.6292576747880855, loss_content: 0.3301921992654532, loss_domain: 0.28426443963823184 redundancy: 0.049336781423591394  acc_d1: 92.71111297607422 acc_d2: 81.67259979248047 acc_domain: 96.28167724609375\n",
      "\n",
      "epoch: 54, loss: 0.7264428046387685, loss_content: 0.337108115809904, loss_domain: 0.37644250040322963 redundancy: 0.042973950019919535  acc_d1: 92.70000457763672 acc_d2: 82.10631561279297 acc_domain: 95.18119049072266\n",
      "\n",
      "epoch: 55, loss: 0.6352971473210295, loss_content: 0.34007648882311836, loss_domain: 0.27819580364395197 redundancy: 0.056749528975711325  acc_d1: 92.73333740234375 acc_d2: 82.217529296875 acc_domain: 96.03712463378906\n",
      "\n",
      "epoch: 56, loss: 0.7062386908161808, loss_content: 0.34958912547625287, loss_domain: 0.34027601956901415 redundancy: 0.054578499684871086  acc_d1: 92.75555419921875 acc_d2: 82.33985900878906 acc_domain: 95.86483001708984\n",
      "\n",
      "epoch: 57, loss: 0.7213809935139938, loss_content: 0.3580472534391242, loss_domain: 0.345750808401007 redundancy: 0.058609775123251995  acc_d1: 92.78888702392578 acc_d2: 82.3287353515625 acc_domain: 95.87594604492188\n",
      "\n",
      "epoch: 58, loss: 0.7380933228512885, loss_content: 0.36167809493105174, loss_domain: 0.3602152178195161 redundancy: 0.05400001971890599  acc_d1: 92.81111145019531 acc_d2: 82.08407592773438 acc_domain: 95.73143768310547\n",
      "\n",
      "epoch: 59, loss: 0.7357314984563371, loss_content: 0.36785012126808436, loss_domain: 0.35124470534878716 redundancy: 0.05545555465710415  acc_d1: 92.64444732666016 acc_d2: 82.45106506347656 acc_domain: 95.90929412841797\n",
      "\n",
      "epoch: 60, loss: 0.824298295756461, loss_content: 0.36194495499973567, loss_domain: 0.4460530289461915 redundancy: 0.05433436787464249  acc_d1: 92.63333129882812 acc_d2: 82.54003143310547 acc_domain: 95.2423324584961\n",
      "\n",
      "epoch: 61, loss: 0.7376432809191691, loss_content: 0.37034438403559405, loss_domain: 0.34945973481090975 redundancy: 0.059463883645202914  acc_d1: 92.80000305175781 acc_d2: 82.72909545898438 acc_domain: 96.02045440673828\n",
      "\n",
      "epoch: 62, loss: 0.7383419031828222, loss_content: 0.3765902727002829, loss_domain: 0.34446713832062736 redundancy: 0.05761497574005748  acc_d1: 92.86666870117188 acc_d2: 82.48442840576172 acc_domain: 96.24832916259766\n",
      "\n",
      "epoch: 63, loss: 0.8469780263766437, loss_content: 0.37954097188694375, loss_domain: 0.4498673660654417 redundancy: 0.058565626419346095  acc_d1: 92.74444580078125 acc_d2: 82.69573211669922 acc_domain: 95.14228820800781\n",
      "\n",
      "epoch: 64, loss: 0.8535991772799425, loss_content: 0.3829066604375839, loss_domain: 0.4565483594024685 redundancy: 0.047147178012405484  acc_d1: 92.66666412353516 acc_d2: 82.79581451416016 acc_domain: 95.5980453491211\n",
      "\n",
      "epoch: 65, loss: 0.720656695919977, loss_content: 0.38568244181888206, loss_domain: 0.3204497011299704 redundancy: 0.04841520049622361  acc_d1: 92.73333740234375 acc_d2: 82.71797180175781 acc_domain: 96.35393524169922\n",
      "\n",
      "epoch: 66, loss: 0.8215746333901311, loss_content: 0.39253264602640986, loss_domain: 0.41335797099999977 redundancy: 0.0522800600156188  acc_d1: 92.73333740234375 acc_d2: 82.762451171875 acc_domain: 95.40351104736328\n",
      "\n",
      "epoch: 67, loss: 0.8032949470298391, loss_content: 0.389570759215825, loss_domain: 0.3965281314203437 redundancy: 0.05732019771625039  acc_d1: 92.67778015136719 acc_d2: 82.69573211669922 acc_domain: 95.67585754394531\n",
      "\n",
      "epoch: 68, loss: 0.8802370387063899, loss_content: 0.39444059280442517, loss_domain: 0.4706578080503034 redundancy: 0.05046211932064362  acc_d1: 92.65555572509766 acc_d2: 82.10631561279297 acc_domain: 95.13672637939453\n",
      "\n",
      "epoch: 69, loss: 0.8984973006685015, loss_content: 0.3877882722397925, loss_domain: 0.49593325567917085 redundancy: 0.04925255596795133  acc_d1: 92.74444580078125 acc_d2: 81.73932647705078 acc_domain: 94.853271484375\n",
      "\n",
      "epoch: 70, loss: 0.9564984356853324, loss_content: 0.3898551258402811, loss_domain: 0.5536557085497279 redundancy: 0.04329200547752561  acc_d1: 92.95555877685547 acc_d2: 82.3064956665039 acc_domain: 94.51979064941406\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "hyperparams = copy.deepcopy(BASE_HYPERPARAMS)\n",
    "hyperparams.alpha = 1.0\n",
    "hyperparams.beta = 1.0\n",
    "hyperparams.gamma = 0.3\n",
    "hyperparams.n_slice = 100\n",
    "hyperparams.learning_rate = 1e-3\n",
    "hyperparams.batch_size = 128\n",
    "hyperparams.estimator = 'SLICE'\n",
    "hyperparams.n_epochs = 70\n",
    "\n",
    "da = DomainAdaptation(DIM_Z_CONTENT, DIM_Z_DOMAIN, NUM_CLASSES, NUM_DOMAIN, hyperparams).to(DEVICE)\n",
    "\n",
    "da = exp_run(\n",
    "    (m_train_loader, mm_train_loader), (m_test_loader, mm_test_loader),\n",
    "    train, test,\n",
    "    inner_batch_provider, get_model_path,\n",
    "    hyperparams,\n",
    "    device='cuda:0',\n",
    "    model=da,\n",
    "    # scheduler_func=lambda optimizer: torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=20)\n",
    "    scheduler_func=lambda optimizer: torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[50], gamma=0.1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c5201396-ce4a-49ab-9578-efdfb21d6a7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 1.0979798175919224, loss_content: 1.4901455523262561, loss_domain: 0.4815712566946594 redundancy: -174.74740364182162  acc_d1: 42.52222442626953 acc_d2: 33.3073844909668 acc_domain: 86.82747650146484\n",
      "\n",
      "epoch: 1, loss: 1.8145598727212826, loss_content: 0.9155045042575245, loss_domain: 1.0532486249863262 redundancy: -30.838649588571467  acc_d1: 67.45555877685547 acc_d2: 56.516902923583984 acc_domain: 76.0060043334961\n",
      "\n",
      "epoch: 2, loss: 2.579122953011956, loss_content: 1.0795885198552844, loss_domain: 1.4820463204048049 redundancy: 3.497622073536188  acc_d1: 63.57777786254883 acc_d2: 52.09074783325195 acc_domain: 52.97354507446289\n",
      "\n",
      "epoch: 3, loss: 1.1834586699244003, loss_content: 0.6738110386149984, loss_domain: 0.5278251477530305 redundancy: -3.6355011899706344  acc_d1: 76.76667022705078 acc_d2: 64.64635467529297 acc_domain: 81.81969451904297\n",
      "\n",
      "epoch: 4, loss: 1.6870660143838803, loss_content: 0.606261090073787, loss_domain: 1.0945426485907863 redundancy: -2.7475456116904673  acc_d1: 79.21111297607422 acc_d2: 63.03380584716797 acc_domain: 67.5022201538086\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [10], line 15\u001b[0m\n\u001b[1;32m     11\u001b[0m hyperparams\u001b[38;5;241m.\u001b[39minner_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2500\u001b[39m\n\u001b[1;32m     13\u001b[0m da \u001b[38;5;241m=\u001b[39m DomainAdaptation(DIM_Z_CONTENT, DIM_Z_DOMAIN, NUM_CLASSES, NUM_DOMAIN, hyperparams)\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[0;32m---> 15\u001b[0m da \u001b[38;5;241m=\u001b[39m \u001b[43mexp_run\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[43mm_train_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmm_train_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mm_test_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmm_test_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43minner_batch_provider\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_model_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhyperparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDEVICE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mda\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscheduler_func\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlr_scheduler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMultiStepLR\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmilestones\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgamma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/research/infomin/routine.py:19\u001b[0m, in \u001b[0;36mexp_run\u001b[0;34m(train_loaders, test_loaders, train, test, infomin_batch_provider, model_naming, hyperparams, device, model, scheduler_func)\u001b[0m\n\u001b[1;32m     15\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m scheduler_func \u001b[38;5;28;01melse\u001b[39;00m scheduler_func(optimizer)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, hyperparams\u001b[38;5;241m.\u001b[39mn_epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;66;03m# train\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minfomin_batch_provider\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhyperparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduler\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;66;03m# test\u001b[39;00m\n\u001b[1;32m     22\u001b[0m     loss, _ \u001b[38;5;241m=\u001b[39m test(epoch, model, test_loaders, hyperparams)\n",
      "File \u001b[0;32m~/research/infomin/tasks/domain_adaptation.py:125\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(epoch, model, optimizer, loaders, infomin_batch_provider, hyperparams, scheduler)\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;66;03m# clip gradient\u001b[39;00m\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m hyperparams\u001b[38;5;241m.\u001b[39mgrad_clip \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: nn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_value_(model\u001b[38;5;241m.\u001b[39mparameters(), hyperparams\u001b[38;5;241m.\u001b[39mgrad_clip)\n\u001b[0;32m--> 125\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    126\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    128\u001b[0m \u001b[38;5;66;03m# schedule lr\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/qqq/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:65\u001b[0m, in \u001b[0;36m_LRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m instance\u001b[38;5;241m.\u001b[39m_step_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     64\u001b[0m wrapped \u001b[38;5;241m=\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__get__\u001b[39m(instance, \u001b[38;5;28mcls\u001b[39m)\n\u001b[0;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/qqq/lib/python3.9/site-packages/torch/optim/optimizer.py:113\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m--> 113\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/qqq/lib/python3.9/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/qqq/lib/python3.9/site-packages/torch/optim/adam.py:157\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    153\u001b[0m                 max_exp_avg_sqs\u001b[38;5;241m.\u001b[39mappend(state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_exp_avg_sq\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    155\u001b[0m             state_steps\u001b[38;5;241m.\u001b[39mappend(state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m--> 157\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m         \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m         \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m         \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m         \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m         \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m         \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m         \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m         \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m         \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m         \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m         \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m         \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m         \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m         \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/anaconda3/envs/qqq/lib/python3.9/site-packages/torch/optim/adam.py:213\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    211\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 213\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/qqq/lib/python3.9/site-packages/torch/optim/adam.py:305\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable)\u001b[0m\n\u001b[1;32m    303\u001b[0m     denom \u001b[38;5;241m=\u001b[39m (max_exp_avg_sqs[i]\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 305\u001b[0m     denom \u001b[38;5;241m=\u001b[39m (\u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbias_correction2_sqrt\u001b[49m)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[1;32m    307\u001b[0m param\u001b[38;5;241m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mstep_size)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "hyperparams = copy.deepcopy(BASE_HYPERPARAMS)\n",
    "hyperparams.alpha = 1.0\n",
    "hyperparams.beta = 1.0\n",
    "hyperparams.gamma = 0.005\n",
    "hyperparams.estimator = 'CLUB'\n",
    "hyperparams.learning_rate = 1e-3\n",
    "hyperparams.inner_lr = 1e-3\n",
    "hyperparams.inner_epochs = 5\n",
    "hyperparams.inner_batch_size = 2500\n",
    "\n",
    "da = DomainAdaptation(DIM_Z_CONTENT, DIM_Z_DOMAIN, NUM_CLASSES, NUM_DOMAIN, hyperparams).to(DEVICE)\n",
    "\n",
    "da = exp_run(\n",
    "    (m_train_loader, mm_train_loader), (m_test_loader, mm_test_loader),\n",
    "    train, test,\n",
    "    inner_batch_provider, get_model_path,\n",
    "    hyperparams,\n",
    "    device=DEVICE,\n",
    "    model=da,\n",
    "    scheduler_func=lambda optimizer: torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[50], gamma=0.1)\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
