{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fce23035-d496-462a-8e69-fc2b6c92b209",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffda532f-6868-4673-9f38-3579657b7802",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "parent_dir = os.path.abspath('../..')\n",
    "\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.append(parent_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fdaad58f-781e-4005-855c-aa46f13c108f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "from torchvision.utils import make_grid\n",
    "import matplotlib.pyplot as plt\n",
    "import utils_os\n",
    "import mi\n",
    "\n",
    "from datasets.dsprite import load_dsprite\n",
    "\n",
    "\n",
    "torch.manual_seed(time.time())\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8382a226-f5d8-4573-a38b-7a870a801e45",
   "metadata": {},
   "source": [
    "# Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f30964b9-5c68-4584-9633-6ecfdf744325",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'mps' \n",
    "\n",
    "class Hyperparams(utils_os.ConfigDict):\n",
    "\n",
    "    def __init__(self): \n",
    "        self.device = DEVICE\n",
    "        self.learning_rate = 5e-4\n",
    "        self.n_epochs = 50\n",
    "        self.grad_clip = None\n",
    "        self.batch_size = 256\n",
    "        self.image_shape = (32, 32)\n",
    "        self.update_inner_every_num_epoch = 2\n",
    "\n",
    "BASE_HYPERPARAMS = Hyperparams()\n",
    "\n",
    "ROOT = '../..'\n",
    "MODEL_SAVE_DIR = f'{ROOT}/results/dsprite'\n",
    "DATASET_DIR = f'{ROOT}/data'\n",
    "\n",
    "NUM_COLOR_CHANNELS = 1\n",
    "NUM_CLASSES = 3\n",
    "\n",
    "DIM_LEARNT = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78fd87b9-13f1-41ca-b1b6-8710ee87490d",
   "metadata": {},
   "source": [
    "# Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5787e45-6803-40b5-9d88-f8908c8144bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loader2tensor finished.\n",
      "loader2tensor finished.\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    train_set, val_set, test_set,\n",
    "    train_loader, val_loader, test_loader,\n",
    ") = load_dsprite(\n",
    "    save_dir=DATASET_DIR,\n",
    "    downsample_pct=1.0,\n",
    "    batch_size=BASE_HYPERPARAMS.batch_size,\n",
    "    img_size=BASE_HYPERPARAMS.image_shape[0],\n",
    "    filter_scale=[3, 5],\n",
    "    target_transform_class=transforms.Compose([\n",
    "        lambda x: torch.LongTensor(x)[1],\n",
    "        lambda x: F.one_hot(x, num_classes=NUM_CLASSES).float()\n",
    "    ]),\n",
    ")\n",
    "\n",
    "\n",
    "def loader2tensor(dataloader):\n",
    "    all_data, all_target = [], []\n",
    "    for batch_idx, (data, target) in enumerate(dataloader):\n",
    "        all_data.append(data)\n",
    "        all_target.append(target)\n",
    "    print('loader2tensor finished.')\n",
    "    return torch.cat(all_data, dim=0), torch.cat(all_target, dim=0)\n",
    "\n",
    "\n",
    "all_data, all_label = loader2tensor(train_loader)\n",
    "all_data_test, all_label_test = loader2tensor(test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77196779-b73d-4bc9-be94-7455b9275652",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tasks.disentanglement.experiment import Autoencoder, train, test, exp_run\n",
    "\n",
    "\n",
    "def inner_batch_provider():\n",
    "    idx = torch.randperm(len(all_data))\n",
    "    xx, yy = all_data[idx[:10000]], all_label[idx[:10000]]\n",
    "    return xx, yy\n",
    "\n",
    "\n",
    "def get_model_path(hyperparams, epoch=None):\n",
    "    if epoch is not None:\n",
    "        return f'{MODEL_SAVE_DIR}/{hyperparams.estimator}/model_{hyperparams.get_name(\"alpha\", \"beta\", \"gamma\")}_{epoch}'\n",
    "    return f'{MODEL_SAVE_DIR}/{hyperparams.estimator}/model_{hyperparams.get_name(\"alpha\", \"beta\", \"gamma\")}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f81571-abc9-482e-a70f-1001aba9b1e6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dim learnt 256 dim expert 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/weihaosun/opt/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:2066: UserWarning: The operator 'aten::mish.out' is not currently supported on the MPS backend and will fall back to run on the CPU. This may have performance implications. (Triggered internally at  /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/mps/MPSFallback.mm:11.)\n",
      "  return torch._C._nn.mish_(input)\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "hyperparams = copy.deepcopy(BASE_HYPERPARAMS)\n",
    "hyperparams.alpha = 1.0\n",
    "hyperparams.beta = 1.0\n",
    "hyperparams.gamma = 0.1\n",
    "hyperparams.n_slice = 250\n",
    "hyperparams.estimator = 'SLICE'\n",
    "\n",
    "autoencoder = Autoencoder(DIM_LEARNT, NUM_CLASSES, hyperparams, num_color_channel=1).to(DEVICE)\n",
    "\n",
    "autoencoder = exp_run(\n",
    "    (train_loader, ), (test_loader, ),\n",
    "    train, test,\n",
    "    inner_batch_provider, get_model_path,\n",
    "    hyperparams,\n",
    "    device=DEVICE,\n",
    "    model=autoencoder,\n",
    "    scheduler_func=lambda optimizer: torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[40], gamma=0.1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8366ca19-df7e-4838-ac20-ff1765380db7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torchvision\n",
    "\n",
    "def swap_z1_z2(network):\n",
    "    cate_a = 0\n",
    "    cate_b = 1\n",
    "    \n",
    "    data, target = all_data_test[0:1000], all_label_test[0:1000]\n",
    "    \n",
    "    x_a = data[target.argmax(dim=1) == cate_a]\n",
    "    x_b = data[target.argmax(dim=1) == cate_b]\n",
    "    y = target\n",
    "    num_sample = 300\n",
    "\n",
    "    x_a = x_a[:num_sample]\n",
    "    x_b = x_b[:num_sample]\n",
    "    \n",
    "    y_a = y[target.argmax(dim=1) == cate_a][:num_sample]\n",
    "    y_b = y[target.argmax(dim=1) == cate_b][:num_sample]\n",
    "    \n",
    "    z_a, _ = network.separate_feature(x_a)\n",
    "    z_b, _ = network.separate_feature(x_b)\n",
    "    \n",
    "    rx_zayb = network.decode(z_a, y_b).detach()\n",
    "    rx_zbya = network.decode(z_b, y_a).detach()\n",
    "    rx_a = network.decode(z_a, y_a).detach()\n",
    "    rx_b = network.decode(z_b, y_b).detach()\n",
    "    \n",
    "    mse = torch.norm((x_a - rx_a).view(len(x_a), -1), dim=1)\n",
    "    print('mse=', mse.mean())\n",
    "    \n",
    "    num_sample = 8\n",
    "\n",
    "    x_a = x_a[:num_sample]\n",
    "    x_b = x_b[:num_sample]\n",
    "    \n",
    "    z_a = z_a[:num_sample]\n",
    "    z_b = z_b[:num_sample]\n",
    "    rx_zayb = rx_zayb[:num_sample]\n",
    "    rx_zbya = rx_zbya[:num_sample]\n",
    "    rx_a = rx_a[:num_sample]\n",
    "    rx_b = rx_b[:num_sample]\n",
    "\n",
    "    plt.figure(figsize=(20, 2))\n",
    "    fz = 14\n",
    "\n",
    "    # GT y\n",
    "    ax = plt.subplot(2, 3, 1)\n",
    "    ax.set_title(r\"$x_1$\", fontsize=fz)\n",
    "    ax.set_axis_off()\n",
    "    grid_img = torchvision.utils.make_grid(x_a, nrow=8)\n",
    "    plt.imshow(grid_img.permute(1, 2, 0))\n",
    "    \n",
    "    ax = plt.subplot(2, 3, 4)\n",
    "    ax.set_title(r\"$x_2$\", fontsize=fz)\n",
    "    ax.set_axis_off()\n",
    "    grid_img = torchvision.utils.make_grid(x_b, nrow=8)\n",
    "    plt.imshow(grid_img.permute(1, 2, 0))\n",
    "        \n",
    "    ax = plt.subplot(2, 3, 6)\n",
    "    ax.set_title(r\"$G(z_2, y_1)$\", fontsize=fz)\n",
    "    ax.set_axis_off()\n",
    "    grid_img = torchvision.utils.make_grid(rx_zbya, nrow=8)\n",
    "    plt.imshow(grid_img.permute(1, 2, 0))\n",
    "    \n",
    "    ax = plt.subplot(2, 3, 3)\n",
    "    ax.set_title(r\"$G(z_1, y_2)$\", fontsize=fz)\n",
    "    ax.set_axis_off()\n",
    "    grid_img = torchvision.utils.make_grid(rx_zayb, nrow=8)\n",
    "    plt.imshow(grid_img.permute(1, 2, 0))\n",
    "    \n",
    "    ax = plt.subplot(2, 3, 2)\n",
    "    ax.set_title(r\"$G(z_1, y_1)$\", fontsize=fz)\n",
    "    ax.set_axis_off()\n",
    "    grid_img = torchvision.utils.make_grid(rx_a, nrow=8)\n",
    "    plt.imshow(grid_img.permute(1, 2, 0))\n",
    "    \n",
    "    ax = plt.subplot(2, 3, 5)\n",
    "    ax.set_title(r\"$G(z_2, y_2)$\", fontsize=fz)\n",
    "    ax.set_axis_off()\n",
    "    grid_img = torchvision.utils.make_grid(rx_b, nrow=8)\n",
    "    plt.imshow(grid_img.permute(1, 2, 0))\n",
    "    \n",
    "    \n",
    "swap_z1_z2(autoencoder.to('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29425442-3ac2-4161-b2d2-91abd6a4f545",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams = copy.deepcopy(BASE_HYPERPARAMS)\n",
    "hyperparams.alpha = 1.0\n",
    "hyperparams.beta = 1.0\n",
    "hyperparams.gamma = 7.0\n",
    "hyperparams.n_slice = 250\n",
    "hyperparams.estimator = 'CLUB'\n",
    "hyperparams.learning_rate = 1e-4\n",
    "hyperparams.inner_lr = 1e-3\n",
    "hyperparams.inner_epochs = 10\n",
    "hyperparams.inner_batch_size = 5000\n",
    "\n",
    "autoencoder = Autoencoder(DIM_LEARNT, NUM_CLASSES, hyperparams, num_color_channel=1).to(DEVICE)\n",
    "\n",
    "autoencoder = exp_run(\n",
    "    (train_loader, ), (test_loader, ),\n",
    "    train, test,\n",
    "    inner_batch_provider, get_model_path,\n",
    "    hyperparams,\n",
    "    device='cuda:0',\n",
    "    model=autoencoder,\n",
    "    scheduler_func=lambda optimizer: torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[40], gamma=0.1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d017b15-ac61-482c-92dd-b0b9b9accc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "swap_z1_z2(autoencoder.to('cpu'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
